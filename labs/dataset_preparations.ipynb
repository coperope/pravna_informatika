{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "Stopwords = set(stopwords.words('english'))\n",
    "wordlemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    try:\n",
    "        with open(name + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f)    \n",
    "    except FileNotFoundError as e:\n",
    "        return False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn initial dataset to csv from json files\n",
    "\n",
    "import csv\n",
    "with open('../data/dataset/dataset_filtered.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"title\", \"header\", \"recitals\", \"main_body\", \"attachments\", \"concepts\"])\n",
    "\n",
    "    directory = os.fsencode('../data/dataset/train_filtered/')\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".json\"): \n",
    "            concepts = []\n",
    "            title = \"\"\n",
    "            header = \"\"\n",
    "            recitals = \"\"\n",
    "            main_body = \"\"\n",
    "            attachments = \"\"\n",
    "\n",
    "            with open('../data/dataset/train_filtered/' + str(filename), encoding='utf-8') as json_file:\n",
    "                data = json.load(json_file)\n",
    "                concepts = data[\"concepts\"]\n",
    "                title = data[\"title\"]\n",
    "                header = data[\"header\"]\n",
    "                recitals = data[\"recitals\"]\n",
    "                main_body = '\\n'.join(data[\"main_body\"])\n",
    "                attachments = data[\"attachments\"]\n",
    "                writer.writerow([title, header, recitals, main_body, attachments, concepts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "True\n",
      "122882\n",
      "451\n",
      "3612\n"
     ]
    }
   ],
   "source": [
    "# FIND No. OF OCCURANCES OF EACH CONCEPT AND FILTER OUT ONLY ONES THAT APPEAR IN MORE THAT 10 DOCS.\n",
    "# Note: the dictionary with this data (key: CONCEPT_ID, value: NUMBER_OF_OCCURANCES) is saved in file 'dict_concept_num'.\n",
    "# Note 2: Filtered dict contains 1289 concepts.\n",
    "\n",
    "dict = {}\n",
    "directory = os.fsencode('../data/dataset/train/')\n",
    "counter = 0  \n",
    "print(counter)\n",
    "dict = load_obj('helper_objects/dict_concept_num')\n",
    "if dict:\n",
    "    print('True')\n",
    "else:\n",
    "    dict = {}\n",
    "    for file in os.listdir(directory):\n",
    "        counter = counter + 1\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".json\"): \n",
    "            concepts = []\n",
    "            title = \"\"\n",
    "            header = \"\"\n",
    "            recitals = \"\"\n",
    "            main_body = \"\"\n",
    "            attachments = \"\"\n",
    "\n",
    "            with open('../data/dataset/train/' + str(filename), encoding='utf-8') as json_file:\n",
    "                data = json.load(json_file)\n",
    "                concepts = data[\"concepts\"]\n",
    "                for concept in concepts:\n",
    "                    if concept in dict:\n",
    "                      dict[concept] = dict[concept] + 1\n",
    "                    else:\n",
    "                      dict[concept] = 1\n",
    "    #save_obj(dict,'helper_objects/dict_concept_num')\n",
    "filtered_dict = {k:v for (k,v) in dict.items() if v > 10}   \n",
    "sum_values = 0\n",
    "for (k,v) in dict.items():\n",
    "    sum_values = sum_values + v\n",
    "print(sum_values)\n",
    "\n",
    "print(len(filtered_dict))\n",
    "print(len(dict))\n",
    "save_obj(filtered_dict, \"helper_objects/dict_concept_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451\n"
     ]
    }
   ],
   "source": [
    "# REMOVE PREVIOUSLY REMOVED CONCEPTS FROM DOCUMENTS and CREATE NEW DATASET WITH DOCUMENTS WITH LEFTOVER CONCEPTS. \n",
    "# Note: this removes concepts that have exclusively concepts that are removed in the prevous step.\n",
    "# Note 2: this removes only ~ 300-400 docs.\n",
    "\n",
    "directory = os.fsencode('../data/dataset/train/')\n",
    "dict_rez = {}\n",
    "counter = 0\n",
    "for file in os.listdir(directory):\n",
    "    counter = counter + 1\n",
    "    filename = os.fsdecode(file)\n",
    "#     if counter == 7:\n",
    "#         break\n",
    "    if filename.endswith(\".json\"): \n",
    "        concepts = []\n",
    "        title = \"\"\n",
    "        header = \"\"\n",
    "        recitals = \"\"\n",
    "        main_body = \"\"\n",
    "        attachments = \"\"\n",
    "\n",
    "        with open('../data/dataset/train/' + str(filename), encoding='utf-8') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            concepts = data[\"concepts\"]\n",
    "            rez_concepts = []\n",
    "            for concept in concepts:\n",
    "                if concept in filtered_dict:\n",
    "                  rez_concepts.append(concept)\n",
    "                  dict_rez[concept] = dict_rez[concept] + 1 if concept in dict_rez else 1\n",
    "            if len(rez_concepts) > 0:\n",
    "                data[\"concepts\"] = rez_concepts\n",
    "                with open('../data/dataset/train_filtered/' + str(filename), 'w') as outfile:\n",
    "                    json.dump(data, outfile)\n",
    "print(len(dict_rez))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
